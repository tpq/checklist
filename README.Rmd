---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# Overview

<!-- badges: start -->
<!-- badges: end -->

At its core, checklist is a base-R implementation of recursive feature
elimination (RFE), a search algorithm that seeks to identify a minimal set of
features that minimise the total loss of a fitted model. The algorithm is
written in a generic way so that it can use any user-defined loss function
that takes the arguments 'x' and 'y' and returns a numeric value.

The package offers two additional features:

(1) Feature elimination can be sped up by computing the loss on a
proportion ('prop') of the data, or by eliminating several ('k')
features per step.

(2) Custom functions enable RFE to be used to select features for a linear
model where all features are binary variables and all coefficients are integers,
yielding a model that is simply a point-based checklist.

You can install the development version of checklist like so:

```{r, eval = FALSE}
devtools::install_github("tpq/checklist")
library(checklist)
```

```{r, echo = FALSE}
library(checklist)
```

## Recursive Feature Elimination

Let us make some data where features 2 and 7 are very important, while features
1, 9, and 10 are not important at all.

```{r}
M <- matrix(runif(1000), 100, 10)
colnames(M) <- paste0("V", 1:10)
y <- M[,2] + M[,7] + rowSums(M[,2:8]) + 2*runif(100)
```

We can eliminate unimportant features, but first we need to define the model
and its loss. For this, we will use linear regression with mean absolute error.

```{r}
my_loss <- function(x, y) {
  m = lm(y ~ ., data.frame(x, y))
  yhat = predict(m, x)
  sum(abs(y - yhat))
}
```

Now, let us eliminate unimportant features.

```{r example}
res <- rfe(M, y, my_loss)
res
```

Judging from above, we can do pretty well with just 4 of the 10 features.

## Making A Checklist

In checklist, we define a checklist as a linear model with integer
coefficients. When coefficients are integers, it is feasible for a human
to calculate predictions "by hand". When the number of features are few,
the model is easy to interpret and thus easy to implement in practice.

We wrote this tool to make it easy to make a checklist from your data. The tool
will calculate the integer coefficients and eliminate the unimportant features.
Ideally, the features used in a checklist are binary,
but it is up to you to make them binary.
Here are some binary data.

```{r}
M <- matrix(sample(0:1, 1000, replace = T), 100, 10)
colnames(M) <- paste0("V", 1:10)
y <- M[,2] + M[,7] + rowSums(M[,2:8]) + 2*runif(100)
```

A checklist is fit like a typical model. We use non-parametric correlation as
the loss because we expect that the final tallied score will have a different
scale than the target variable. Note that the checklist has no Intercept.
Use `u` to tune the size of the integers in the checklist.

```{r}
chk = checklist(M, y, u = 5)
yhat = predict(chk, M)
cor(yhat, y, method = "spearman")
```

The package contains a checklist loss for use with RFE. The loss is 1 minus the
Spearman correlation between the tallied score and the target variable.

```{r}
checklist_loss <-
  function(x, y) {
    chk = checklist(x, y, u = 5)
    yhat = predict(chk, x)
    1 - cor(yhat, y, method = "spearman")
  }
```

We can now eliminate features.

```{r}
res <- rfe(M, y, checklist_loss)
res
```

We can now fit a 4 feature model.

```{r}
best4 <- res$Column[1:4]
M_reduced <- M[,best4]
chk <- checklist(M_reduced, y, u = 5)
yhat = predict(chk, M_reduced)
cor(yhat, y, method = "spearman")
```

The '$points' slot stores the points per feature.

```{r}
chk$points
```
